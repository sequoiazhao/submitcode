{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensor_01.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sequoiazhao/submitcode/blob/master/Tensor_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wtz5zaFsgQq",
        "colab_type": "text"
      },
      "source": [
        "*   二次代价函数\n",
        "\n",
        "\n",
        "$$C =\\frac{1}{2n} \\sum_{x} (y(x) - a^l(x))^2$$\n",
        "\n",
        "*   其中 $a=\\delta(z)$,激活函数，z是网络的权值之和\n",
        "$z=\\sum{W_j*X_j}+b$\n",
        "\n",
        "*   梯度下降：\n",
        "$$\\frac{\\partial C}{\\partial w_j}=(a-y)\\delta'(z)x$$ \n",
        "$$\\frac{\\partial C}{\\partial b}=(a-y)\\delta'(z)$$ \n",
        "\n",
        "对w和b求偏导数\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bF6EZl07sDdp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "3e947410-66bd-4159-afd7-0ba602206e79"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "# get data\n",
        "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)\n",
        "\n",
        "# 批次大小\n",
        "batch_size = 100\n",
        "\n",
        "n_batch = mnist.train.num_examples // batch_size\n",
        "\n",
        "# x = tf.placeholder(tf.float32, [None, 784])\n",
        "# y = tf.placeholder(tf.float32, [None, 10])\n",
        "#\n",
        "# W = tf.Variable(tf.zeros([784, 10]))\n",
        "# b = tf.Variable(tf.zeros([10]))\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None, 784])\n",
        "y = tf.placeholder(tf.float32, [None, 10])\n",
        "\n",
        "WL1 = tf.Variable(tf.random_normal([784, 2000]))\n",
        "bL1 = tf.Variable(tf.zeros([1, 2000]))\n",
        "Wx_L1 = tf.matmul(x, WL1) + bL1\n",
        "# active\n",
        "Wx_L1_ac = tf.nn.tanh(Wx_L1)\n",
        "\n",
        "# WL2 = tf.Variable(tf.random_normal([1000, 500]))\n",
        "# bL2 = tf.Variable(tf.zeros([1, 500]))\n",
        "# Wx_L2 = tf.matmul(Wx_L1_ac, WL2) + bL2\n",
        "# # active\n",
        "# Wx_L2_ac = tf.nn.tanh(Wx_L2)\n",
        "#\n",
        "W = tf.Variable(tf.zeros([2000, 10]))\n",
        "b = tf.Variable(tf.zeros([10]))\n",
        "\n",
        "prediction = tf.nn.softmax(tf.matmul(Wx_L1_ac, W) + b)\n",
        "\n",
        "# 二次代价函数\n",
        "loss = tf.reduce_mean(tf.square(y - prediction))\n",
        "# 梯度下降\n",
        "train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# 结果存放\n",
        "correct_prediction = tf.equal(tf.arg_max(y, 1), tf.arg_max(prediction, 1))\n",
        "\n",
        "# 先将bool型转为float\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "config = tf.ConfigProto(allow_soft_placement=True)\n",
        "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.7)\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(21):\n",
        "        for batch in range(n_batch):\n",
        "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "            sess.run(train_step, feed_dict={x: batch_xs, y: batch_ys})\n",
        "        #\n",
        "        acc = sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
        "        print(\"iter \" + str(epoch) + \",Testing Accuracy \" + str(acc))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "iter 0,Testing Accuracy 0.9257\n",
            "iter 1,Testing Accuracy 0.9365\n",
            "iter 2,Testing Accuracy 0.9418\n",
            "iter 3,Testing Accuracy 0.9433\n",
            "iter 4,Testing Accuracy 0.9458\n",
            "iter 5,Testing Accuracy 0.9478\n",
            "iter 6,Testing Accuracy 0.949\n",
            "iter 7,Testing Accuracy 0.9497\n",
            "iter 8,Testing Accuracy 0.9509\n",
            "iter 9,Testing Accuracy 0.9508\n",
            "iter 10,Testing Accuracy 0.9522\n",
            "iter 11,Testing Accuracy 0.9529\n",
            "iter 12,Testing Accuracy 0.9536\n",
            "iter 13,Testing Accuracy 0.954\n",
            "iter 14,Testing Accuracy 0.9548\n",
            "iter 15,Testing Accuracy 0.9544\n",
            "iter 16,Testing Accuracy 0.9554\n",
            "iter 17,Testing Accuracy 0.9551\n",
            "iter 18,Testing Accuracy 0.9544\n",
            "iter 19,Testing Accuracy 0.9556\n",
            "iter 20,Testing Accuracy 0.9556\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HUaXV8DxQGe",
        "colab_type": "text"
      },
      "source": [
        "交叉熵代价函数：\n",
        "\n",
        "$$C =-\\frac{1}{n} \\sum_{x} [y\\ln a+(1-y)\\ln (1-a)]$$\n",
        "\n",
        "梯度下降：\n",
        "$$\\frac{\\partial C}{\\partial w_j}=\\frac{1}{n}\\sum_x x_j(\\delta(z)-y)$$ \n",
        "$$\\frac{\\partial C}{\\partial b}=\\frac{1}{n}(\\delta(z)-y)$$ \n",
        "\n",
        "好处：权值和偏置值的调整与$\\delta'(z) $无关，另外梯度公式中的$(\\delta(z)-y)$表示输出值与实际值的误差，误差越大，梯度就越大。参数w,b调整就越快，训练速度也快。\n",
        "\n",
        "*  如果输出神经元是线性的，二次代价函数合适。\n",
        "\n",
        "*   如果输入神经元是S型函数，交叉熵代价函数合适。"
      ]
    }
  ]
}