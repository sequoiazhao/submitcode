{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2019Tensor_0298.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sequoiazhao/submitcode/blob/master/2019Tensor_0298.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4775-_l8bRpa",
        "colab_type": "code",
        "outputId": "8f480716-e3f9-4a86-ecb4-8def03c93c83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1017
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "\n",
        "# get data\n",
        "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)\n",
        "\n",
        "# 批次大小\n",
        "batch_size = 100\n",
        "\n",
        "n_batch = mnist.train.num_examples // batch_size\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None, 784])\n",
        "y = tf.placeholder(tf.float32, [None, 10])\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "lr = tf.Variable(0.001, dtype=tf.float32)\n",
        "\n",
        "# create network\n",
        "\n",
        "W1 = tf.Variable(tf.truncated_normal([784, 500], stddev=0.1))\n",
        "b1 = tf.Variable(tf.zeros([500]) + 0.1)\n",
        "\n",
        "L1 = tf.nn.tanh(tf.matmul(x, W1) + b1)\n",
        "\n",
        "L1_drop = tf.nn.dropout(L1, keep_prob)\n",
        "\n",
        "W2 = tf.Variable(tf.truncated_normal([500, 300], stddev=0.1))\n",
        "b2 = tf.Variable(tf.zeros([300]) + 0.1)\n",
        "\n",
        "L2 = tf.nn.tanh(tf.matmul(L1_drop, W2) + b2)\n",
        "L2_drop = tf.nn.dropout(L2, keep_prob)\n",
        "\n",
        "W3 = tf.Variable(tf.truncated_normal([300, 10], stddev=0.1))\n",
        "b3 = tf.Variable(tf.zeros([10]) + 0.1)\n",
        "\n",
        "prediction = tf.nn.softmax(tf.matmul(L2_drop, W3) + b3)\n",
        "\n",
        "\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=prediction))\n",
        "\n",
        "train_step = tf.train.AdamOptimizer(lr).minimize(loss)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(prediction, 1))\n",
        "\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(51):\n",
        "        sess.run(tf.assign(lr, 0.001 * (0.95 ** epoch)))\n",
        "        for batch in range(n_batch):\n",
        "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "            sess.run(train_step, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.0})\n",
        "\n",
        "        learning_rate = sess.run(lr)\n",
        "        acc = sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels, keep_prob: 1.0})\n",
        "        print(\"Iter\" + str(epoch) + \",Testing Accuracy=\" + str(acc) + \", Learning Rate=\" + str(learning_rate))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "Iter0,Testing Accuracy=0.9483, Learning Rate=0.001\n",
            "Iter1,Testing Accuracy=0.9625, Learning Rate=0.00095\n",
            "Iter2,Testing Accuracy=0.9667, Learning Rate=0.0009025\n",
            "Iter3,Testing Accuracy=0.972, Learning Rate=0.000857375\n",
            "Iter4,Testing Accuracy=0.9715, Learning Rate=0.00081450626\n",
            "Iter5,Testing Accuracy=0.9733, Learning Rate=0.0007737809\n",
            "Iter6,Testing Accuracy=0.9739, Learning Rate=0.0007350919\n",
            "Iter7,Testing Accuracy=0.9745, Learning Rate=0.0006983373\n",
            "Iter8,Testing Accuracy=0.9765, Learning Rate=0.0006634204\n",
            "Iter9,Testing Accuracy=0.9768, Learning Rate=0.0006302494\n",
            "Iter10,Testing Accuracy=0.9774, Learning Rate=0.0005987369\n",
            "Iter11,Testing Accuracy=0.9791, Learning Rate=0.0005688001\n",
            "Iter12,Testing Accuracy=0.9791, Learning Rate=0.0005403601\n",
            "Iter13,Testing Accuracy=0.9784, Learning Rate=0.0005133421\n",
            "Iter14,Testing Accuracy=0.9792, Learning Rate=0.000487675\n",
            "Iter15,Testing Accuracy=0.9794, Learning Rate=0.00046329122\n",
            "Iter16,Testing Accuracy=0.9766, Learning Rate=0.00044012666\n",
            "Iter17,Testing Accuracy=0.9786, Learning Rate=0.00041812033\n",
            "Iter18,Testing Accuracy=0.9789, Learning Rate=0.00039721432\n",
            "Iter19,Testing Accuracy=0.9805, Learning Rate=0.0003773536\n",
            "Iter20,Testing Accuracy=0.9812, Learning Rate=0.00035848594\n",
            "Iter21,Testing Accuracy=0.982, Learning Rate=0.00034056162\n",
            "Iter22,Testing Accuracy=0.9782, Learning Rate=0.00032353355\n",
            "Iter23,Testing Accuracy=0.9812, Learning Rate=0.00030735688\n",
            "Iter24,Testing Accuracy=0.9814, Learning Rate=0.000291989\n",
            "Iter25,Testing Accuracy=0.9807, Learning Rate=0.00027738957\n",
            "Iter26,Testing Accuracy=0.9815, Learning Rate=0.0002635201\n",
            "Iter27,Testing Accuracy=0.9815, Learning Rate=0.00025034408\n",
            "Iter28,Testing Accuracy=0.9816, Learning Rate=0.00023782688\n",
            "Iter29,Testing Accuracy=0.9814, Learning Rate=0.00022593554\n",
            "Iter30,Testing Accuracy=0.9817, Learning Rate=0.00021463877\n",
            "Iter31,Testing Accuracy=0.9804, Learning Rate=0.00020390682\n",
            "Iter32,Testing Accuracy=0.9823, Learning Rate=0.00019371149\n",
            "Iter33,Testing Accuracy=0.982, Learning Rate=0.0001840259\n",
            "Iter34,Testing Accuracy=0.9817, Learning Rate=0.00017482461\n",
            "Iter35,Testing Accuracy=0.9821, Learning Rate=0.00016608338\n",
            "Iter36,Testing Accuracy=0.9826, Learning Rate=0.00015777921\n",
            "Iter37,Testing Accuracy=0.9811, Learning Rate=0.00014989026\n",
            "Iter38,Testing Accuracy=0.9803, Learning Rate=0.00014239574\n",
            "Iter39,Testing Accuracy=0.9818, Learning Rate=0.00013527596\n",
            "Iter40,Testing Accuracy=0.9822, Learning Rate=0.00012851215\n",
            "Iter41,Testing Accuracy=0.9819, Learning Rate=0.00012208655\n",
            "Iter42,Testing Accuracy=0.9809, Learning Rate=0.00011598222\n",
            "Iter43,Testing Accuracy=0.9809, Learning Rate=0.00011018311\n",
            "Iter44,Testing Accuracy=0.9818, Learning Rate=0.000104673956\n",
            "Iter45,Testing Accuracy=0.9811, Learning Rate=9.944026e-05\n",
            "Iter46,Testing Accuracy=0.9819, Learning Rate=9.446825e-05\n",
            "Iter47,Testing Accuracy=0.9822, Learning Rate=8.974483e-05\n",
            "Iter48,Testing Accuracy=0.9818, Learning Rate=8.525759e-05\n",
            "Iter49,Testing Accuracy=0.9817, Learning Rate=8.099471e-05\n",
            "Iter50,Testing Accuracy=0.982, Learning Rate=7.6944976e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrgsmEz-1LAr",
        "colab_type": "text"
      },
      "source": [
        "可视化，构建tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mf8BRyRAbZgM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "# get data\n",
        "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)\n",
        "\n",
        "\n",
        "\n",
        "# 批次大小\n",
        "batch_size = 100\n",
        "\n",
        "n_batch = mnist.train.num_examples // batch_size\n",
        "\n",
        "#定义命名空间\n",
        "with tf.name_scope(\"input\"):\n",
        "    x = tf.placeholder(tf.float32, [None, 784],name=\"x-input\")\n",
        "    y = tf.placeholder(tf.float32, [None, 10],name='y-input')\n",
        "\n",
        "#\n",
        "with tf.name_scope(\"layer\"):\n",
        "    with tf.name_scope(\"weights\"):\n",
        "        W = tf.Variable(tf.zeros([784, 10]),name=\"W\")\n",
        "    with tf.name_scope(\"biases\"):\n",
        "        b = tf.Variable(tf.zeros([10])+0.1,name=\"b\")\n",
        "    with tf.name_scope(\"wx_plus_b\"):\n",
        "        wx_plus_b = tf.matmul(x,W)+b\n",
        "    with tf.name_scope(\"softmax\"):\n",
        "        prediction = tf.nn.softmax(wx_plus_b)\n",
        "\n",
        "# 二次代价函数\n",
        "# loss = tf.reduce_mean(tf.square(y - prediction))\n",
        "with tf.name_scope(\"loss\"):\n",
        "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=prediction))\n",
        "# 梯度下降\n",
        "# train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n",
        "with tf.name_scope(\"train\"):\n",
        "    train_step = tf.train.AdamOptimizer(1e-3).minimize(loss)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# 结果存放\n",
        "with tf.name_scope(\"accuracy\"):\n",
        "    with tf.name_scope(\"prediction\"):\n",
        "        correct_prediction = tf.equal(tf.arg_max(y, 1), tf.arg_max(prediction, 1))\n",
        "\n",
        "    # 先将bool型转为float\n",
        "    with tf.name_scope(\"acc\"):\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "config = tf.ConfigProto(allow_soft_placement=True)\n",
        "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.7)\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    writer = tf.summary.FileWriter('logs/',sess.graph)\n",
        "    for epoch in range(2):\n",
        "        for batch in range(n_batch):\n",
        "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "            sess.run(train_step, feed_dict={x: batch_xs, y: batch_ys})\n",
        "        #\n",
        "        acc = sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
        "        print(\"iter \" + str(epoch) + \",Testing Accuracy \" + str(acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SsNmJM27CPf",
        "colab_type": "text"
      },
      "source": [
        "各种参数设置visible"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NsYMbtW7FNr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "# get data\n",
        "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)\n",
        "\n",
        "# 批次大小\n",
        "batch_size = 100\n",
        "\n",
        "n_batch = mnist.train.num_examples // batch_size\n",
        "\n",
        "\n",
        "# 参数概要\n",
        "def variable_summaries(var):\n",
        "    with tf.name_scope('summaries'):\n",
        "        mean = tf.reduce_mean(var)\n",
        "        tf.summary.scalar(\"mean\", mean)\n",
        "        with tf.name_scope('stddev'):\n",
        "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
        "        tf.summary.scalar(\"stddev\", stddev)\n",
        "        tf.summary.scalar(\"max\", tf.reduce_max(var))\n",
        "        tf.summary.scalar(\"min\", tf.reduce_min(var))\n",
        "        tf.summary.histogram(\"histogram\", var)\n",
        "\n",
        "\n",
        "# 定义命名空间\n",
        "with tf.name_scope(\"input\"):\n",
        "    x = tf.placeholder(tf.float32, [None, 784], name=\"x-input\")\n",
        "    y = tf.placeholder(tf.float32, [None, 10], name='y-input')\n",
        "\n",
        "#\n",
        "with tf.name_scope(\"layer\"):\n",
        "    with tf.name_scope(\"weights\"):\n",
        "        W = tf.Variable(tf.truncated_normal([784, 10]), name=\"W\")\n",
        "        variable_summaries(W)\n",
        "    with tf.name_scope(\"biases\"):\n",
        "        b = tf.Variable(tf.zeros([10]) + 0.1, name=\"b\")\n",
        "        variable_summaries(b)\n",
        "    with tf.name_scope(\"wx_plus_b\"):\n",
        "        wx_plus_b = tf.matmul(x, W) + b\n",
        "    with tf.name_scope(\"softmax\"):\n",
        "        prediction = tf.nn.softmax(wx_plus_b)\n",
        "\n",
        "# 二次代价函数\n",
        "# loss = tf.reduce_mean(tf.square(y - prediction))\n",
        "with tf.name_scope(\"loss\"):\n",
        "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=prediction))\n",
        "    tf.summary.scalar(\"loss\", loss)\n",
        "# 梯度下降\n",
        "# train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n",
        "with tf.name_scope(\"train\"):\n",
        "    train_step = tf.train.AdamOptimizer(1e-3).minimize(loss)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# 结果存放\n",
        "with tf.name_scope(\"accuracy\"):\n",
        "    with tf.name_scope(\"prediction\"):\n",
        "        correct_prediction = tf.equal(tf.arg_max(y, 1), tf.arg_max(prediction, 1))\n",
        "\n",
        "    # 先将bool型转为float\n",
        "    with tf.name_scope(\"acc\"):\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "        tf.summary.scalar(\"accuracy\", accuracy)\n",
        "\n",
        "config = tf.ConfigProto(allow_soft_placement=True)\n",
        "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.7)\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "# 合并所有summary\n",
        "merged = tf.summary.merge_all()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    writer = tf.summary.FileWriter('logs/', sess.graph)\n",
        "    for epoch in range(51):\n",
        "        for batch in range(n_batch):\n",
        "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "            summary, _ = sess.run([merged, train_step], feed_dict={x: batch_xs, y: batch_ys})\n",
        "\n",
        "        writer.add_summary(summary, epoch)\n",
        "        acc = sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
        "        print(\"iter \" + str(epoch) + \",Testing Accuracy \" + str(acc))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z79AgWVZMiUv",
        "colab_type": "text"
      },
      "source": [
        "设置分类数据的可视化效果!\n",
        "![替代文字](https://lh3.google.com/u/0/d/1HcJLpLLj6NqqylKSjwL33Vx_C6eLXg9y=w1920-h855-iv1)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx2gLTsuDLam",
        "colab_type": "code",
        "outputId": "6ef503fd-2526-4532-956d-ad283a4b9d68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "from tensorflow.contrib.tensorboard.plugins import projector\n",
        "\n",
        "# get data\n",
        "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)\n",
        "\n",
        "max_steps = 1001\n",
        "\n",
        "image_num = 3000\n",
        "\n",
        "sess = tf.Session()\n",
        "\n",
        "embedding = tf.Variable(tf.stack(mnist.test.images[:image_num]), trainable=False, name=\"embedding\")\n",
        "\n",
        "\n",
        "# 参数概要\n",
        "def variable_summaries(var):\n",
        "    with tf.name_scope('summaries'):\n",
        "        mean = tf.reduce_mean(var)\n",
        "        tf.summary.scalar(\"mean\", mean)\n",
        "        with tf.name_scope('stddev'):\n",
        "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
        "        tf.summary.scalar(\"stddev\", stddev)\n",
        "        tf.summary.scalar(\"max\", tf.reduce_max(var))\n",
        "        tf.summary.scalar(\"min\", tf.reduce_min(var))\n",
        "        tf.summary.histogram(\"histogram\", var)\n",
        "\n",
        "\n",
        "with tf.name_scope(\"input\"):\n",
        "    x = tf.placeholder(tf.float32, [None, 784], name=\"x-input\")\n",
        "    y = tf.placeholder(tf.float32, [None, 10], name='y-input')\n",
        "\n",
        "with tf.name_scope(\"input_reshpae\"):\n",
        "    image_shaped_input = tf.reshape(x, [-1, 28, 28, 1])\n",
        "    tf.summary.image('input', image_shaped_input, 10)\n",
        "\n",
        "with tf.name_scope(\"layer\"):\n",
        "    with tf.name_scope(\"weights\"):\n",
        "        W = tf.Variable(tf.truncated_normal([784, 10]), name=\"W\")\n",
        "        variable_summaries(W)\n",
        "    with tf.name_scope(\"biases\"):\n",
        "        b = tf.Variable(tf.zeros([10]) + 0.1, name=\"b\")\n",
        "        variable_summaries(b)\n",
        "    with tf.name_scope(\"wx_plus_b\"):\n",
        "        wx_plus_b = tf.matmul(x, W) + b\n",
        "    with tf.name_scope(\"softmax\"):\n",
        "        prediction = tf.nn.softmax(wx_plus_b)\n",
        "# 二次代价函数\n",
        "# loss = tf.reduce_mean(tf.square(y - prediction))\n",
        "with tf.name_scope(\"loss\"):\n",
        "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=prediction))\n",
        "    tf.summary.scalar(\"loss\", loss)\n",
        "with tf.name_scope(\"train\"):\n",
        "    train_step = tf.train.AdamOptimizer(1e-3).minimize(loss)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# 结果存放\n",
        "with tf.name_scope(\"accuracy\"):\n",
        "    with tf.name_scope(\"prediction\"):\n",
        "        correct_prediction = tf.equal(tf.arg_max(y, 1), tf.arg_max(prediction, 1))\n",
        "\n",
        "    # 先将bool型转为float\n",
        "    with tf.name_scope(\"acc\"):\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "        tf.summary.scalar(\"accuracy\", accuracy)\n",
        "\n",
        "# metadata\n",
        "Dir = \"/code/remotePy/com/Tensor/\"\n",
        "if tf.gfile.Exists(Dir + 'projector/projector/metadata.tsv'):\n",
        "    tf.gfile.DeleteRecursively(Dir + 'projector/projector/metadata.tsv')\n",
        "\n",
        "with open(Dir + 'projector/projector/metadata.tsv', 'w') as f:\n",
        "    labels = sess.run(tf.argmax(mnist.test.labels[:], 1))\n",
        "    for i in range(image_num):\n",
        "        f.write(str(labels[i]) + '\\n')\n",
        "\n",
        "\n",
        "config = tf.ConfigProto(allow_soft_placement=True)\n",
        "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.7)\n",
        "config.gpu_options.allow_growth = True\n",
        "# merged\n",
        "merged = tf.summary.merge_all()\n",
        "\n",
        "projector_writer = tf.summary.FileWriter(Dir + 'projector/projector', sess.graph)\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "config = projector.ProjectorConfig()\n",
        "embed = config.embeddings.add()\n",
        "embed.tensor_name = embedding.name\n",
        "embed.metadata_path = Dir + \"projector/projector/metadata.tsv\"\n",
        "embed.sprite.image_path = Dir + \"projector/data/mnist_10k_sprite.png\"\n",
        "embed.sprite.single_image_dim.extend([28, 28])\n",
        "projector.visualize_embeddings(projector_writer, config)\n",
        "\n",
        "sess.run(init)\n",
        "\n",
        "for i in range(max_steps):\n",
        "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
        "    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
        "    run_metadata = tf.RunMetadata()\n",
        "    summary, _ = sess.run([merged, train_step], feed_dict={x: batch_xs, y: batch_ys}, options=run_options,\n",
        "                          run_metadata=run_metadata)\n",
        "    projector_writer.add_run_metadata(run_metadata, 'step%03d' % i)\n",
        "    projector_writer.add_summary(summary, i)\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        acc = sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
        "        print(\"Iter\" + str(i) + \",Testing Accuracy=\" + str(acc))\n",
        "\n",
        "saver.save(sess, Dir + 'projector/projector/a_model.ckpt', global_step=max_steps)\n",
        "projector_writer.close()\n",
        "sess.close()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-1-1e9be68510d7>:7: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-1-1e9be68510d7>:53: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-1-1e9be68510d7>:63: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.math.argmax` instead\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1e9be68510d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDeleteRecursively\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'projector/projector/metadata.tsv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'projector/projector/metadata.tsv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/code/remotePy/com/Tensor/projector/projector/metadata.tsv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAEa-dhcOkt6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_9blujSOlDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}